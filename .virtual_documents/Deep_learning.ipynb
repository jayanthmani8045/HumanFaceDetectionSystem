





import cv2


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image, ImageDraw
import os
import random
import warnings
warnings.filterwarnings('ignore')





images_file_path = 'images'
csv = 'faces.csv'


# Step 1A: Load and examine CSV structure
print("1. LOADING CSV DATA")
print("-" * 50)

# Load the CSV file
df = pd.read_csv(csv)

# Basic dataset information
print(f"Dataset Shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
print(f"Data types:\n{df.dtypes}")
print(f"\nFirst 5 rows:")
print(df.head())

print(f"\nBasic statistics:")
print(df.describe())

# Check for missing values
print(f"\nMissing values:")
print(df.isnull().sum())





# Step 1B: Dataset Overview Analysis
print("2. DATASET OVERVIEW ANALYSIS")
print("-" * 50)

# Total annotations vs unique images
total_annotations = len(df)
unique_images = df['image_name'].nunique()
total_image_files = len([f for f in os.listdir(images_file_path) if f.endswith('.jpg')])  # Adjust path as needed

print(f"Total annotations (faces): {total_annotations}")
print(f"Unique images in CSV: {unique_images}")
print(f"Total .jpg files in directory: {total_image_files}")

# Images with multiple faces
faces_per_image = df['image_name'].value_counts()
multiple_faces = faces_per_image[faces_per_image > 1]

print(f"\nImages with multiple faces: {len(multiple_faces)}")
print(f"Images with single face: {len(faces_per_image[faces_per_image == 1])}")
print(f"Maximum faces in one image: {faces_per_image.max()}")
print(f"Average faces per image: {faces_per_image.mean():.2f}")

if len(multiple_faces) > 0:
    print(f"\nTop 5 images with most faces:")
    print(multiple_faces.head())





# Step 1C: File Existence Verification
print("3. FILE EXISTENCE VERIFICATION")
print("-" * 50)

# Check if all images in CSV actually exist
image_directory = images_file_path  # Adjust this path to your images directory
existing_images = []
missing_images = []

for image_name in df['image_name'].unique():
    image_path = os.path.join(image_directory, image_name)
    if os.path.exists(image_path):
        existing_images.append(image_name)
    else:
        missing_images.append(image_name)

print(f"Images found: {len(existing_images)}")
print(f"Missing images: {len(missing_images)}")

if missing_images:
    print(f"Missing image files: {missing_images[:5]}...")  # Show first 5 missing files

# Create a clean dataset with only existing images
df_clean = df[df['image_name'].isin(existing_images)]
print(f"Clean dataset shape (after removing missing images): {df_clean.shape}")





print("\n" + "="*70 + "\n")

# Step 1D: Data Quality Checks
print("4. DATA QUALITY CHECKS")
print("-" * 50)

# Check for invalid bounding box coordinates
invalid_boxes = []

# Check if x0 < x1 and y0 < y1 (valid bounding box)
invalid_x = df_clean[df_clean['x0'] >= df_clean['x1']]
invalid_y = df_clean[df_clean['y0'] >= df_clean['y1']]

print(f"Invalid bounding boxes (x0 >= x1): {len(invalid_x)}")
print(f"Invalid bounding boxes (y0 >= y1): {len(invalid_y)}")

# Check if bounding boxes are within image boundaries
def check_bbox_bounds(row):
    x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1']
    width, height = row['width'], row['height']

    out_of_bounds = (x0 < 0 or y0 < 0 or x1 > width or y1 > height)
    return out_of_bounds

df_clean['out_of_bounds'] = df_clean.apply(check_bbox_bounds, axis=1)
out_of_bounds_count = df_clean['out_of_bounds'].sum()

print(f"Bounding boxes outside image boundaries: {out_of_bounds_count}")

# Calculate bounding box dimensions
df_clean['bbox_width'] = df_clean['x1'] - df_clean['x0']
df_clean['bbox_height'] = df_clean['y1'] - df_clean['y0']
df_clean['bbox_area'] = df_clean['bbox_width'] * df_clean['bbox_height']

# Check for very small faces
small_faces = df_clean[(df_clean['bbox_width'] < 30) | (df_clean['bbox_height'] < 30)]
print(f"Very small faces (width or height < 30px): {len(small_faces)}")

# Check for very large faces (might be annotation errors)
large_faces = df_clean[(df_clean['bbox_width'] > df_clean['width'] * 0.8) |
                       (df_clean['bbox_height'] > df_clean['height'] * 0.8)]
print(f"Very large faces (>80% of image): {len(large_faces)}")

print("\n" + "="*70 + "\n")





# Step 1E: Summary Statistics
print("5. SUMMARY STATISTICS")
print("-" * 50)

print(f"FINAL CLEAN DATASET SUMMARY:")
print(f"‚Ä¢ Total valid annotations: {len(df_clean)}")
print(f"‚Ä¢ Unique images: {df_clean['image_name'].nunique()}")
print(f"‚Ä¢ Images with single face: {len(df_clean['image_name'].value_counts()[df_clean['image_name'].value_counts() == 1])}")
print(f"‚Ä¢ Images with multiple faces: {len(df_clean['image_name'].value_counts()[df_clean['image_name'].value_counts() > 1])}")
print(f"‚Ä¢ Average face size: {df_clean['bbox_width'].mean():.1f} x {df_clean['bbox_height'].mean():.1f} pixels")
print(f"‚Ä¢ Face size range: {df_clean['bbox_width'].min():.0f}-{df_clean['bbox_width'].max():.0f} x {df_clean['bbox_height'].min():.0f}-{df_clean['bbox_height'].max():.0f} pixels")


# Save clean dataset
df_clean.to_csv('clean_annotations.csv', index=False)
print(f"\n‚úì Clean dataset saved as 'clean_annotations.csv'")
print(f"‚úì Phase 1 completed successfully!")


# Set up plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Create a visualization of faces per image distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
faces_per_image.hist(bins=range(1, faces_per_image.max() + 2), alpha=0.7, edgecolor='black')
plt.xlabel('Number of Faces per Image')
plt.ylabel('Frequency')
plt.title('Distribution of Faces per Image')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(df_clean['bbox_width'], df_clean['bbox_height'], alpha=0.6)
plt.xlabel('Face Width (pixels)')
plt.ylabel('Face Height (pixels)')
plt.title('Face Dimensions Distribution')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('phase1_basic_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nüìä Visualization saved as 'phase1_basic_analysis.png'")











# Load the clean dataset
df = pd.read_csv('clean_annotations.csv')

# Step 2A: Image Dimension Analysis
print("1. IMAGE DIMENSION ANALYSIS")
print("-" * 50)

# Basic image dimension statistics
print("Image Dimensions Statistics:")
print(f"Width  - Min: {df['width'].min()}, Max: {df['width'].max()}, Mean: {df['width'].mean():.1f}, Std: {df['width'].std():.1f}")
print(f"Height - Min: {df['height'].min()}, Max: {df['height'].max()}, Mean: {df['height'].mean():.1f}, Std: {df['height'].std():.1f}")

# Image aspect ratios
df['image_aspect_ratio'] = df['width'] / df['height']
print(f"\nImage Aspect Ratios:")
print(f"Min: {df['image_aspect_ratio'].min():.2f}, Max: {df['image_aspect_ratio'].max():.2f}, Mean: {df['image_aspect_ratio'].mean():.2f}")

# Common image sizes
image_sizes = df.groupby(['width', 'height']).size().reset_index(name='count')
image_sizes = image_sizes.sort_values('count', ascending=False)
print(f"\nTop 10 Most Common Image Sizes:")
for i in range(min(10, len(image_sizes))):
    w, h, count = image_sizes.iloc[i]
    print(f"{int(w)}x{int(h)}: {count} images")

print("\n" + "="*70 + "\n")





# Step 2B: Face Size Analysis
print("2. FACE SIZE ANALYSIS")
print("-" * 50)

# Face dimensions (already calculated in Phase 1, but let's recalculate for clarity)
df['face_width'] = df['x1'] - df['x0']
df['face_height'] = df['y1'] - df['y0']
df['face_area'] = df['face_width'] * df['face_height']
df['face_aspect_ratio'] = df['face_width'] / df['face_height']

print("Face Dimensions Statistics:")
print(f"Face Width  - Min: {df['face_width'].min()}, Max: {df['face_width'].max()}, Mean: {df['face_width'].mean():.1f}")
print(f"Face Height - Min: {df['face_height'].min()}, Max: {df['face_height'].max()}, Mean: {df['face_height'].mean():.1f}")
print(f"Face Area   - Min: {df['face_area'].min()}, Max: {df['face_area'].max()}, Mean: {df['face_area'].mean():.1f}")

# Face aspect ratio analysis
print(f"\nFace Aspect Ratios:")
print(f"Min: {df['face_aspect_ratio'].min():.2f}, Max: {df['face_aspect_ratio'].max():.2f}, Mean: {df['face_aspect_ratio'].mean():.2f}")

# Categorize face sizes
def categorize_face_size(area):
    if area < 5000:
        return 'Small'
    elif area < 20000:
        return 'Medium'
    elif area < 50000:
        return 'Large'
    else:
        return 'Very Large'

df['face_size_category'] = df['face_area'].apply(categorize_face_size)
face_size_dist = df['face_size_category'].value_counts()
print(f"\nFace Size Distribution:")
for category, count in face_size_dist.items():
    percentage = (count / len(df)) * 100
    print(f"{category}: {count} ({percentage:.1f}%)")

print("\n" + "="*70 + "\n")





# Step 2C: Face Position Analysis
print("3. FACE POSITION ANALYSIS")
print("-" * 50)

# Calculate face center positions
df['face_center_x'] = (df['x0'] + df['x1']) / 2
df['face_center_y'] = (df['y0'] + df['y1']) / 2

# Normalize face positions (0-1 range)
df['face_center_x_norm'] = df['face_center_x'] / df['width']
df['face_center_y_norm'] = df['face_center_y'] / df['height']

print("Face Position Statistics (Normalized 0-1):")
print(f"X-center - Min: {df['face_center_x_norm'].min():.3f}, Max: {df['face_center_x_norm'].max():.3f}, Mean: {df['face_center_x_norm'].mean():.3f}")
print(f"Y-center - Min: {df['face_center_y_norm'].min():.3f}, Max: {df['face_center_y_norm'].max():.3f}, Mean: {df['face_center_y_norm'].mean():.3f}")

# Face position relative to image
def categorize_position(x_norm, y_norm):
    x_pos = 'left' if x_norm < 0.33 else 'center' if x_norm < 0.67 else 'right'
    y_pos = 'top' if y_norm < 0.33 else 'middle' if y_norm < 0.67 else 'bottom'
    return f"{y_pos}-{x_pos}"

df['position_category'] = df.apply(lambda row: categorize_position(row['face_center_x_norm'], row['face_center_y_norm']), axis=1)
position_dist = df['position_category'].value_counts()
print(f"\nFace Position Distribution:")
for position, count in position_dist.items():
    percentage = (count / len(df)) * 100
    print(f"{position}: {count} ({percentage:.1f}%)")

print("\n" + "="*70 + "\n")





# Step 2D: Face-to-Image Size Ratio Analysis
print("4. FACE-TO-IMAGE SIZE RATIO ANALYSIS")
print("-" * 50)

# Calculate what percentage of image the face occupies
df['image_area'] = df['width'] * df['height']
df['face_to_image_ratio'] = df['face_area'] / df['image_area']

print("Face-to-Image Ratio Statistics:")
print(f"Min: {df['face_to_image_ratio'].min():.4f} ({df['face_to_image_ratio'].min()*100:.2f}%)")
print(f"Max: {df['face_to_image_ratio'].max():.4f} ({df['face_to_image_ratio'].max()*100:.2f}%)")
print(f"Mean: {df['face_to_image_ratio'].mean():.4f} ({df['face_to_image_ratio'].mean()*100:.2f}%)")
print(f"Median: {df['face_to_image_ratio'].median():.4f} ({df['face_to_image_ratio'].median()*100:.2f}%)")

# Categorize face-to-image ratios
def categorize_face_ratio(ratio):
    if ratio < 0.01:
        return 'Tiny (<1%)'
    elif ratio < 0.05:
        return 'Small (1-5%)'
    elif ratio < 0.15:
        return 'Medium (5-15%)'
    elif ratio < 0.30:
        return 'Large (15-30%)'
    else:
        return 'Very Large (>30%)'

df['face_ratio_category'] = df['face_to_image_ratio'].apply(categorize_face_ratio)
ratio_dist = df['face_ratio_category'].value_counts()
print(f"\nFace-to-Image Ratio Distribution:")
for category, count in ratio_dist.items():
    percentage = (count / len(df)) * 100
    print(f"{category}: {count} ({percentage:.1f}%)")

print("\n" + "="*70 + "\n")





# Step 2E: Multi-face Image Analysis
print("5. MULTI-FACE IMAGE ANALYSIS")
print("-" * 50)

# Analyze images with multiple faces
faces_per_image = df.groupby('image_name').size()
multi_face_images = faces_per_image[faces_per_image > 1]

print(f"Multi-face Images Analysis:")
print(f"Images with 2 faces: {len(multi_face_images[multi_face_images == 2])}")
print(f"Images with 3 faces: {len(multi_face_images[multi_face_images == 3])}")
print(f"Images with 4+ faces: {len(multi_face_images[multi_face_images >= 4])}")

# For multi-face images, analyze face size variation
multi_face_df = df[df['image_name'].isin(multi_face_images.index)]
face_size_variation = multi_face_df.groupby('image_name')['face_area'].agg(['min', 'max', 'std']).reset_index()
face_size_variation['size_ratio'] = face_size_variation['max'] / face_size_variation['min']

print(f"\nFace Size Variation in Multi-face Images:")
print(f"Average size ratio (largest/smallest face): {face_size_variation['size_ratio'].mean():.2f}")
print(f"Max size ratio: {face_size_variation['size_ratio'].max():.2f}")

print("\n" + "="*70 + "\n")






# Set up plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
fig_size = (15, 10)

# Create comprehensive visualizations
print("6. CREATING DETAILED VISUALIZATIONS")
print("-" * 50)

# Create a large figure with multiple subplots
fig = plt.figure(figsize=(20, 15))
gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)

# 1. Image dimension distribution
ax1 = fig.add_subplot(gs[0, 0])
ax1.hist(df['width'], bins=50, alpha=0.7, label='Width', edgecolor='black')
ax1.set_xlabel('Image Width (pixels)')
ax1.set_ylabel('Frequency')
ax1.set_title('Image Width Distribution')
ax1.grid(True, alpha=0.3)

ax2 = fig.add_subplot(gs[0, 1])
ax2.hist(df['height'], bins=50, alpha=0.7, label='Height', color='orange', edgecolor='black')
ax2.set_xlabel('Image Height (pixels)')
ax2.set_ylabel('Frequency')
ax2.set_title('Image Height Distribution')
ax2.grid(True, alpha=0.3)

# 2. Image aspect ratio
ax3 = fig.add_subplot(gs[0, 2])
ax3.hist(df['image_aspect_ratio'], bins=50, alpha=0.7, color='green', edgecolor='black')
ax3.set_xlabel('Aspect Ratio (Width/Height)')
ax3.set_ylabel('Frequency')
ax3.set_title('Image Aspect Ratio Distribution')
ax3.grid(True, alpha=0.3)

# 3. Face size distribution
ax4 = fig.add_subplot(gs[0, 3])
face_size_dist.plot(kind='bar', ax=ax4, color='purple')
ax4.set_xlabel('Face Size Category')
ax4.set_ylabel('Count')
ax4.set_title('Face Size Distribution')
ax4.tick_params(axis='x', rotation=45)

# 4. Face dimensions scatter
ax5 = fig.add_subplot(gs[1, 0])
scatter = ax5.scatter(df['face_width'], df['face_height'],
                     c=df['face_area'], cmap='viridis', alpha=0.6, s=10)
ax5.set_xlabel('Face Width (pixels)')
ax5.set_ylabel('Face Height (pixels)')
ax5.set_title('Face Dimensions (colored by area)')
plt.colorbar(scatter, ax=ax5, label='Face Area')

# 5. Face aspect ratio
ax6 = fig.add_subplot(gs[1, 1])
ax6.hist(df['face_aspect_ratio'], bins=50, alpha=0.7, color='red', edgecolor='black')
ax6.set_xlabel('Face Aspect Ratio')
ax6.set_ylabel('Frequency')
ax6.set_title('Face Aspect Ratio Distribution')
ax6.axvline(1.0, color='black', linestyle='--', label='Square (1:1)')
ax6.legend()
ax6.grid(True, alpha=0.3)

# 6. Face position heatmap
ax7 = fig.add_subplot(gs[1, 2])
ax7.hist2d(df['face_center_x_norm'], df['face_center_y_norm'], bins=20, cmap='Blues')
ax7.set_xlabel('Face Center X (normalized)')
ax7.set_ylabel('Face Center Y (normalized)')
ax7.set_title('Face Position Heatmap')

# 7. Face-to-image ratio
ax8 = fig.add_subplot(gs[1, 3])
ax8.hist(df['face_to_image_ratio'] * 100, bins=50, alpha=0.7, color='brown', edgecolor='black')
ax8.set_xlabel('Face-to-Image Ratio (%)')
ax8.set_ylabel('Frequency')
ax8.set_title('Face-to-Image Size Ratio')
ax8.grid(True, alpha=0.3)

# 8. Position distribution
ax9 = fig.add_subplot(gs[2, 0])
position_dist.plot(kind='bar', ax=ax9, color='teal')
ax9.set_xlabel('Position')
ax9.set_ylabel('Count')
ax9.set_title('Face Position Distribution')
ax9.tick_params(axis='x', rotation=45)

# 9. Face ratio categories
ax10 = fig.add_subplot(gs[2, 1])
ratio_dist.plot(kind='bar', ax=ax10, color='gold')
ax10.set_xlabel('Face-to-Image Ratio Category')
ax10.set_ylabel('Count')
ax10.set_title('Face Size Relative to Image')
ax10.tick_params(axis='x', rotation=45)

# 10. Multi-face analysis
ax11 = fig.add_subplot(gs[2, 2])
faces_per_image.hist(bins=range(1, faces_per_image.max() + 2), ax=ax11, alpha=0.7, edgecolor='black')
ax11.set_xlabel('Number of Faces per Image')
ax11.set_ylabel('Frequency')
ax11.set_title('Faces per Image Distribution')
ax11.grid(True, alpha=0.3)

# 11. Face area vs image area
ax12 = fig.add_subplot(gs[2, 3])
ax12.scatter(df['image_area'], df['face_area'], alpha=0.6, s=10)
ax12.set_xlabel('Image Area (pixels¬≤)')
ax12.set_ylabel('Face Area (pixels¬≤)')
ax12.set_title('Face Area vs Image Area')
ax12.set_xscale('log')
ax12.set_yscale('log')
ax12.grid(True, alpha=0.3)

# 12-15. Additional analysis plots
ax13 = fig.add_subplot(gs[3, 0])
df.boxplot(column='face_width', by='face_size_category', ax=ax13)
ax13.set_xlabel('Face Size Category')
ax13.set_ylabel('Face Width')
ax13.set_title('Face Width by Size Category')

ax14 = fig.add_subplot(gs[3, 1])
df.boxplot(column='face_height', by='face_size_category', ax=ax14)
ax14.set_xlabel('Face Size Category')
ax14.set_ylabel('Face Height')
ax14.set_title('Face Height by Size Category')

ax15 = fig.add_subplot(gs[3, 2])
ax15.scatter(df['face_center_x_norm'], df['face_to_image_ratio'], alpha=0.6, s=10)
ax15.set_xlabel('Face Center X (normalized)')
ax15.set_ylabel('Face-to-Image Ratio')
ax15.set_title('Face Position vs Size Ratio')
ax15.grid(True, alpha=0.3)

ax16 = fig.add_subplot(gs[3, 3])
if len(multi_face_images) > 0:
    ax16.hist(face_size_variation['size_ratio'], bins=30, alpha=0.7, edgecolor='black')
    ax16.set_xlabel('Size Ratio (Largest/Smallest Face)')
    ax16.set_ylabel('Frequency')
    ax16.set_title('Face Size Variation in Multi-face Images')
    ax16.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('phase2_image_characteristics.png', dpi=300, bbox_inches='tight')
plt.show()

# Save the enhanced dataset
df.to_csv('enhanced_annotations.csv', index=False)

print("‚úì Enhanced dataset saved as 'enhanced_annotations.csv'")
print("üìä Detailed visualizations saved as 'phase2_image_characteristics.png'")
print("‚úì Phase 2 completed successfully!")


# Summary insights
print("\n" + "="*70)
print("PHASE 2 SUMMARY INSIGHTS:")
print("="*70)
print(f"üìä Image Diversity: Wide range of sizes ({df['width'].min()}-{df['width'].max()}px width)")
print(f"üë§ Face Sizes: Highly variable ({df['face_width'].min()}-{df['face_width'].max()}px width)")
print(f"üìç Face Positions: Most faces in center-middle ({position_dist.index[0]}: {position_dist.iloc[0]} faces)")
print(f"üìè Face-to-Image Ratio: Average {df['face_to_image_ratio'].mean()*100:.1f}% of image area")
print(f"üë• Multi-face Complexity: {len(multi_face_images)} images with multiple faces")
print(f"‚ö° Detection Challenges: {len(df[df['face_to_image_ratio'] < 0.01])} tiny faces (<1% of image)")








# Load the enhanced dataset
df = pd.read_csv('enhanced_annotations.csv')

# Step 3A: Bounding Box Precision Analysis
print("1. BOUNDING BOX PRECISION ANALYSIS")
print("-" * 50)

# Calculate bounding box properties
df['bbox_precision'] = (df['face_width'] * df['face_height']) / ((df['x1'] - df['x0'] + 1) * (df['y1'] - df['y0'] + 1))

# Edge proximity analysis
df['left_edge_dist'] = df['x0'] / df['width']
df['right_edge_dist'] = (df['width'] - df['x1']) / df['width']
df['top_edge_dist'] = df['y0'] / df['height']
df['bottom_edge_dist'] = (df['height'] - df['y1']) / df['height']

# Find faces close to edges (within 5% of image boundary)
edge_threshold = 0.05
df['near_left_edge'] = df['left_edge_dist'] < edge_threshold
df['near_right_edge'] = df['right_edge_dist'] < edge_threshold
df['near_top_edge'] = df['top_edge_dist'] < edge_threshold
df['near_bottom_edge'] = df['bottom_edge_dist'] < edge_threshold
df['near_any_edge'] = df['near_left_edge'] | df['near_right_edge'] | df['near_top_edge'] | df['near_bottom_edge']

edge_faces = df['near_any_edge'].sum()
print(f"Faces near edges (<5% from boundary): {edge_faces} ({edge_faces/len(df)*100:.1f}%)")
print(f"  - Near left edge: {df['near_left_edge'].sum()}")
print(f"  - Near right edge: {df['near_right_edge'].sum()}")
print(f"  - Near top edge: {df['near_top_edge'].sum()}")
print(f"  - Near bottom edge: {df['near_bottom_edge'].sum()}")

# Face truncation analysis (faces that might be cut off)
df['potentially_truncated'] = df['near_any_edge'] & (df['face_to_image_ratio'] > 0.15)
truncated_faces = df['potentially_truncated'].sum()
print(f"\nPotentially truncated faces (large faces near edges): {truncated_faces}")

print("\n" + "="*70 + "\n")





# Step 3B: Annotation Quality Assessment
print("2. ANNOTATION QUALITY ASSESSMENT")
print("-" * 50)

# Check for suspiciously small faces
very_small_faces = df[(df['face_width'] < 50) | (df['face_height'] < 50)]
print(f"Very small faces (<50px in any dimension): {len(very_small_faces)}")

# Check for suspiciously large faces
very_large_faces = df[(df['face_width'] > df['width'] * 0.7) | (df['face_height'] > df['height'] * 0.7)]
print(f"Very large faces (>70% of image in any dimension): {len(very_large_faces)}")

# Aspect ratio outliers
normal_aspect_range = (0.5, 2.0)  # Reasonable face aspect ratio range
aspect_outliers = df[(df['face_aspect_ratio'] < normal_aspect_range[0]) |
                    (df['face_aspect_ratio'] > normal_aspect_range[1])]
print(f"Unusual aspect ratio faces (not between {normal_aspect_range}): {len(aspect_outliers)}")

# Resolution analysis for small faces
low_res_small_faces = df[(df['face_width'] < 64) & (df['face_height'] < 64)]
print(f"Low resolution faces (<64x64): {len(low_res_small_faces)}")

print("\n" + "="*70 + "\n")





# Step 3C: Detection Challenge Classification
print("3. DETECTION CHALLENGE CLASSIFICATION")
print("-" * 50)

def classify_detection_difficulty(row):
    challenges = []

    # Size-based challenges
    if row['face_area'] < 1600:  # <40x40 pixels
        challenges.append('tiny')
    elif row['face_area'] < 6400:  # <80x80 pixels
        challenges.append('small')

    # Position-based challenges
    if row['near_any_edge']:
        challenges.append('edge')

    # Aspect ratio challenges
    if row['face_aspect_ratio'] < 0.6 or row['face_aspect_ratio'] > 1.7:
        challenges.append('unusual_aspect')

    # Resolution challenges
    if row['face_to_image_ratio'] < 0.005:  # <0.5% of image
        challenges.append('low_resolution')

    # Multi-face context challenges
    faces_in_image = df[df['image_name'] == row['image_name']]
    if len(faces_in_image) > 1:
        # Check if this face is much smaller than others in same image
        other_faces = faces_in_image[faces_in_image.index != row.name]
        if len(other_faces) > 0:
            size_ratio = row['face_area'] / other_faces['face_area'].max()
            if size_ratio < 0.25:  # Much smaller than largest face
                challenges.append('scale_variation')

    return challenges

# Apply challenge classification
df['challenges'] = df.apply(classify_detection_difficulty, axis=1)
df['num_challenges'] = df['challenges'].apply(len)
df['challenge_types'] = df['challenges'].apply(lambda x: ','.join(x) if x else 'none')

# Count different challenge types
challenge_counts = {}
for challenges in df['challenges']:
    for challenge in challenges:
        challenge_counts[challenge] = challenge_counts.get(challenge, 0) + 1

print("Detection Challenge Distribution:")
print(f"No challenges: {len(df[df['num_challenges'] == 0])} ({len(df[df['num_challenges'] == 0])/len(df)*100:.1f}%)")
for challenge, count in sorted(challenge_counts.items()):
    print(f"{challenge}: {count} ({count/len(df)*100:.1f}%)")

# Multi-challenge faces
multi_challenge = df[df['num_challenges'] > 1]
print(f"\nFaces with multiple challenges: {len(multi_challenge)} ({len(multi_challenge)/len(df)*100:.1f}%)")

print("\n" + "="*70 + "\n")






# Step 3D: Dataset Balance Analysis
print("4. DATASET BALANCE ANALYSIS")
print("-" * 50)

# Analyze distribution across different dimensions
print("Challenge Type Balance:")
easy_faces = df[df['num_challenges'] == 0]
medium_faces = df[df['num_challenges'] == 1]
hard_faces = df[df['num_challenges'] > 1]

print(f"Easy faces (no challenges): {len(easy_faces)} ({len(easy_faces)/len(df)*100:.1f}%)")
print(f"Medium faces (1 challenge): {len(medium_faces)} ({len(medium_faces)/len(df)*100:.1f}%)")
print(f"Hard faces (2+ challenges): {len(hard_faces)} ({len(hard_faces)/len(df)*100:.1f}%)")

# Size distribution balance
size_categories = df['face_size_category'].value_counts()
print(f"\nSize Category Balance:")
for category, count in size_categories.items():
    print(f"{category}: {count} ({count/len(df)*100:.1f}%)")

# Position distribution balance
position_categories = df['position_category'].value_counts()
print(f"\nPosition Balance (Top 5):")
for position, count in position_categories.head().items():
    print(f"{position}: {count} ({count/len(df)*100:.1f}%)")

print("\n" + "="*70 + "\n")





# Step 3E: Preprocessing Recommendations
print("5. PREPROCESSING RECOMMENDATIONS")
print("-" * 50)

# Image size standardization analysis
unique_sizes = df.groupby(['width', 'height']).size().reset_index(name='count')
print(f"Unique image sizes: {len(unique_sizes)}")

# Recommend target size based on face size distribution
face_size_percentiles = np.percentile(df['face_width'], [25, 50, 75, 90, 95])
print(f"\nFace width percentiles: {face_size_percentiles}")

# Recommend input size that preserves most face detail
min_face_size_target = 32  # Minimum face size after resizing
recommended_input_sizes = []
for percentile in [75, 90, 95]:
    face_size_at_percentile = np.percentile(df['face_width'], percentile)
    scale_factor = min_face_size_target / face_size_at_percentile
    avg_image_size = df[['width', 'height']].mean().mean()
    recommended_size = int(avg_image_size * scale_factor)
    recommended_input_sizes.append((percentile, recommended_size))

print(f"\nRecommended input sizes (to preserve {min_face_size_target}px min face size):")
for percentile, size in recommended_input_sizes:
    print(f"  For {percentile}th percentile faces: {size}px")

# Data augmentation recommendations
print(f"\nData Augmentation Recommendations:")
print(f"  ‚úì Horizontal flip: Safe (faces are symmetric)")
print(f"  ‚úì Scale/Zoom: Needed (wide size variation)")
print(f"  ‚úì Brightness/Contrast: Recommended")
print(f"  ‚ö† Rotation: Use sparingly (faces are mostly upright)")
print(f"  ‚úì Crop variations: Important (many edge cases)")

print("\n" + "="*70 + "\n")






# Set up plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Create comprehensive visualizations
print("6. CREATING CHALLENGE ANALYSIS VISUALIZATIONS")
print("-" * 50)

fig = plt.figure(figsize=(20, 15))
gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)

# 1. Challenge distribution
ax1 = fig.add_subplot(gs[0, 0])
challenge_data = pd.Series(challenge_counts)
challenge_data.plot(kind='bar', ax=ax1, color='coral')
ax1.set_xlabel('Challenge Type')
ax1.set_ylabel('Count')
ax1.set_title('Detection Challenge Distribution')
ax1.tick_params(axis='x', rotation=45)

# 2. Number of challenges per face
ax2 = fig.add_subplot(gs[0, 1])
df['num_challenges'].hist(bins=range(6), ax=ax2, alpha=0.7, edgecolor='black')
ax2.set_xlabel('Number of Challenges')
ax2.set_ylabel('Frequency')
ax2.set_title('Challenges per Face')
ax2.grid(True, alpha=0.3)

# 3. Edge proximity analysis
ax3 = fig.add_subplot(gs[0, 2])
edge_data = pd.Series({
    'Left': df['near_left_edge'].sum(),
    'Right': df['near_right_edge'].sum(),
    'Top': df['near_top_edge'].sum(),
    'Bottom': df['near_bottom_edge'].sum()
})
edge_data.plot(kind='bar', ax=ax3, color='lightblue')
ax3.set_xlabel('Edge')
ax3.set_ylabel('Count')
ax3.set_title('Faces Near Edges')
ax3.tick_params(axis='x', rotation=0)

# 4. Face size vs challenges
ax4 = fig.add_subplot(gs[0, 3])
ax4.scatter(df['face_area'], df['num_challenges'], alpha=0.6, s=10)
ax4.set_xlabel('Face Area (pixels¬≤)')
ax4.set_ylabel('Number of Challenges')
ax4.set_title('Face Size vs Challenge Count')
ax4.set_xscale('log')
ax4.grid(True, alpha=0.3)

# 5. Aspect ratio outliers
ax5 = fig.add_subplot(gs[1, 0])
ax5.hist(df['face_aspect_ratio'], bins=50, alpha=0.7, edgecolor='black')
ax5.axvline(0.5, color='red', linestyle='--', label='Outlier boundaries')
ax5.axvline(2.0, color='red', linestyle='--')
ax5.set_xlabel('Face Aspect Ratio')
ax5.set_ylabel('Frequency')
ax5.set_title('Face Aspect Ratio Distribution')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 6. Face-to-image ratio vs challenges
ax6 = fig.add_subplot(gs[1, 1])
ax6.scatter(df['face_to_image_ratio']*100, df['num_challenges'], alpha=0.6, s=10)
ax6.set_xlabel('Face-to-Image Ratio (%)')
ax6.set_ylabel('Number of Challenges')
ax6.set_title('Face Size Ratio vs Challenges')
ax6.grid(True, alpha=0.3)

# 7. Position heatmap colored by challenge count
ax7 = fig.add_subplot(gs[1, 2])
scatter = ax7.scatter(df['face_center_x_norm'], df['face_center_y_norm'],
                     c=df['num_challenges'], cmap='Reds', alpha=0.6, s=15)
ax7.set_xlabel('Face Center X (normalized)')
ax7.set_ylabel('Face Center Y (normalized)')
ax7.set_title('Face Position Colored by Challenge Count')
plt.colorbar(scatter, ax=ax7, label='Challenges')

# 8. Difficulty distribution
ax8 = fig.add_subplot(gs[1, 3])
difficulty_data = pd.Series({
    'Easy': len(easy_faces),
    'Medium': len(medium_faces),
    'Hard': len(hard_faces)
})
difficulty_data.plot(kind='pie', ax=ax8, autopct='%1.1f%%', colors=['lightgreen', 'orange', 'red'])
ax8.set_title('Detection Difficulty Distribution')

# 9. Box plot of face sizes by challenge count
ax9 = fig.add_subplot(gs[2, 0])
df.boxplot(column='face_area', by='num_challenges', ax=ax9)
ax9.set_xlabel('Number of Challenges')
ax9.set_ylabel('Face Area (pixels¬≤)')
ax9.set_title('Face Size by Challenge Count')
ax9.set_yscale('log')

# 10. Edge distance distribution
ax10 = fig.add_subplot(gs[2, 1])
edge_distances = pd.concat([
    df['left_edge_dist'], df['right_edge_dist'],
    df['top_edge_dist'], df['bottom_edge_dist']
])
ax10.hist(edge_distances, bins=50, alpha=0.7, edgecolor='black')
ax10.axvline(0.05, color='red', linestyle='--', label='Edge threshold')
ax10.set_xlabel('Distance from Edge (normalized)')
ax10.set_ylabel('Frequency')
ax10.set_title('Face Distance from Edges')
ax10.legend()
ax10.grid(True, alpha=0.3)

# 11. Challenge type co-occurrence matrix
ax11 = fig.add_subplot(gs[2, 2])
challenge_types = ['tiny', 'small', 'edge', 'unusual_aspect', 'low_resolution', 'scale_variation']
cooccurrence = np.zeros((len(challenge_types), len(challenge_types)))
for challenges in df['challenges']:
    for i, type1 in enumerate(challenge_types):
        for j, type2 in enumerate(challenge_types):
            if type1 in challenges and type2 in challenges:
                cooccurrence[i, j] += 1

sns.heatmap(cooccurrence, xticklabels=challenge_types, yticklabels=challenge_types,
            annot=True, fmt='g', ax=ax11, cmap='Blues')
ax11.set_title('Challenge Co-occurrence Matrix')
ax11.tick_params(axis='x', rotation=45)
ax11.tick_params(axis='y', rotation=0)

# 12. Face size recommendations
ax12 = fig.add_subplot(gs[2, 3])
ax12.hist(df['face_width'], bins=50, alpha=0.7, edgecolor='black', label='Face widths')
for percentile, size in recommended_input_sizes:
    face_size_at_percentile = np.percentile(df['face_width'], percentile)
    ax12.axvline(face_size_at_percentile, linestyle='--',
                label=f'{percentile}th percentile: {face_size_at_percentile:.0f}px')
ax12.set_xlabel('Face Width (pixels)')
ax12.set_ylabel('Frequency')
ax12.set_title('Face Width Distribution with Percentiles')
ax12.legend()
ax12.grid(True, alpha=0.3)

# Bottom row - sample images with annotations
print("\n7. CREATING SAMPLE IMAGE VISUALIZATIONS")
print("-" * 50)

# Function to visualize sample images with bounding boxes
def visualize_sample_images(df, num_samples=8):
    # Select diverse samples
    sample_indices = []

    # Get samples from different challenge categories
    categories = ['easy', 'medium', 'hard']
    samples_per_category = num_samples // 3

    easy_samples = df[df['num_challenges'] == 0].sample(min(samples_per_category, len(easy_faces)))
    medium_samples = df[df['num_challenges'] == 1].sample(min(samples_per_category, len(medium_faces)))
    hard_samples = df[df['num_challenges'] > 1].sample(min(samples_per_category, len(hard_faces)))

    samples = pd.concat([easy_samples, medium_samples, hard_samples])

    # Create subplot grid for sample images
    fig_samples = plt.figure(figsize=(20, 10))

    for idx, (_, row) in enumerate(samples.iterrows()):
        if idx >= 8:  # Limit to 8 samples
            break

        ax = fig_samples.add_subplot(2, 4, idx + 1)

        try:
            # Load and display image
            img_path = row['image_name']
            if os.path.exists(img_path):
                img = Image.open(img_path)

                # Draw bounding box
                draw = ImageDraw.Draw(img)
                x0, y0, x1, y1 = int(row['x0']), int(row['y0']), int(row['x1']), int(row['y1'])

                # Color code by difficulty
                if row['num_challenges'] == 0:
                    color = 'green'
                elif row['num_challenges'] == 1:
                    color = 'orange'
                else:
                    color = 'red'

                draw.rectangle([x0, y0, x1, y1], outline=color, width=3)

                ax.imshow(img)
                ax.set_title(f"{row['image_name'][:15]}...\n"
                           f"Size: {int(row['face_width'])}x{int(row['face_height'])}\n"
                           f"Challenges: {row['num_challenges']} ({row['challenge_types'][:20]}...)")
                ax.axis('off')
            else:
                ax.text(0.5, 0.5, f"Image not found:\n{img_path}",
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f"Missing: {row['image_name']}")

        except Exception as e:
            ax.text(0.5, 0.5, f"Error loading:\n{str(e)}",
                   ha='center', va='center', transform=ax.transAxes)

    plt.tight_layout()
    plt.savefig('phase3_sample_images.png', dpi=300, bbox_inches='tight')
    plt.show()

    return len(samples)

# Visualize sample images
try:
    num_visualized = visualize_sample_images(df)
    print(f"‚úì Visualized {num_visualized} sample images")
except Exception as e:
    print(f"‚ö† Could not create sample visualizations: {e}")

plt.tight_layout()
plt.savefig('phase3_challenge_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# Save enhanced dataset with challenge analysis
df.to_csv('challenge_analyzed_annotations.csv', index=False)

print("‚úì Challenge analysis dataset saved as 'challenge_analyzed_annotations.csv'")
print("üìä Challenge visualizations saved as 'phase3_challenge_analysis.png'")
if 'num_visualized' in locals():
    print("üñºÔ∏è Sample images saved as 'phase3_sample_images.png'")
print("‚úì Phase 3 completed successfully!")


# Final summary
print("\n" + "="*70)
print("PHASE 3 SUMMARY - DETECTION CHALLENGES:")
print("="*70)
print(f"üéØ Dataset Difficulty: {len(easy_faces)/len(df)*100:.1f}% Easy, {len(medium_faces)/len(df)*100:.1f}% Medium, {len(hard_faces)/len(df)*100:.1f}% Hard")
print(f"‚ö†Ô∏è Main Challenges: {max(challenge_counts, key=challenge_counts.get)} ({max(challenge_counts.values())} faces)")
print(f"üè∑Ô∏è Edge Cases: {edge_faces} faces near boundaries")
print(f"üìè Recommended Input Size: {recommended_input_sizes[1][1]}px (for 90th percentile faces)")
print(f"üîç Critical Issues: {len(very_small_faces)} very small faces, {len(aspect_outliers)} unusual aspects")
print("="*70)








from PIL import Image, ImageDraw, ImageEnhance
import cv2
from collections import Counter


# Load the challenge-analyzed dataset
df = pd.read_csv('challenge_analyzed_annotations.csv')





# Step 4A: Image Quality Assessment
print("1. IMAGE QUALITY ASSESSMENT")
print("-" * 50)

def assess_image_quality(sample_size=100):
    """Assess quality of a sample of images"""
    quality_metrics = {
        'brightness': [],
        'contrast': [],
        'sharpness': [],
        'resolution_score': []
    }

    # Sample images for quality assessment
    sample_images = df['image_name'].unique()[:sample_size]
    processed_count = 0

    for img_name in sample_images:
        try:
            if os.path.exists(img_name):
                # Load image
                img = Image.open(img_name)
                img_cv = cv2.imread(img_name)

                if img_cv is not None:
                    # Convert to grayscale for analysis
                    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)

                    # Brightness (mean pixel value)
                    brightness = np.mean(gray)
                    quality_metrics['brightness'].append(brightness)

                    # Contrast (standard deviation of pixel values)
                    contrast = np.std(gray)
                    quality_metrics['contrast'].append(contrast)

                    # Sharpness (Laplacian variance)
                    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                    quality_metrics['sharpness'].append(laplacian_var)

                    # Resolution score (based on face size in image)
                    face_data = df[df['image_name'] == img_name]
                    avg_face_area = face_data['face_area'].mean()
                    img_area = img.width * img.height
                    resolution_score = avg_face_area / img_area
                    quality_metrics['resolution_score'].append(resolution_score)

                    processed_count += 1

        except Exception as e:
            continue

    print(f"Processed {processed_count} images for quality assessment")

    if processed_count > 0:
        print(f"\nImage Quality Metrics (Sample of {processed_count} images):")
        print(f"Brightness - Mean: {np.mean(quality_metrics['brightness']):.1f}, Std: {np.std(quality_metrics['brightness']):.1f}")
        print(f"Contrast - Mean: {np.mean(quality_metrics['contrast']):.1f}, Std: {np.std(quality_metrics['contrast']):.1f}")
        print(f"Sharpness - Mean: {np.mean(quality_metrics['sharpness']):.1f}, Std: {np.std(quality_metrics['sharpness']):.1f}")
        print(f"Resolution Score - Mean: {np.mean(quality_metrics['resolution_score']):.4f}, Std: {np.std(quality_metrics['resolution_score']):.4f}")

        # Identify quality issues
        low_contrast_threshold = 30
        low_sharpness_threshold = 100
        low_brightness_threshold = 50
        high_brightness_threshold = 200

        low_contrast = sum(1 for c in quality_metrics['contrast'] if c < low_contrast_threshold)
        low_sharpness = sum(1 for s in quality_metrics['sharpness'] if s < low_sharpness_threshold)
        low_brightness = sum(1 for b in quality_metrics['brightness'] if b < low_brightness_threshold)
        high_brightness = sum(1 for b in quality_metrics['brightness'] if b > high_brightness_threshold)

        print(f"\nQuality Issues Detected:")
        print(f"Low contrast images: {low_contrast} ({low_contrast/processed_count*100:.1f}%)")
        print(f"Low sharpness images: {low_sharpness} ({low_sharpness/processed_count*100:.1f}%)")
        print(f"Too dark images: {low_brightness} ({low_brightness/processed_count*100:.1f}%)")
        print(f"Too bright images: {high_brightness} ({high_brightness/processed_count*100:.1f}%)")

    return quality_metrics

# Assess image quality
quality_data = assess_image_quality(100)

print("\n" + "="*70 + "\n")





# Step 4B: Optimal Input Size Analysis
print("2. OPTIMAL INPUT SIZE ANALYSIS")
print("-" * 50)

def calculate_optimal_input_size():
    """Calculate optimal input size based on multiple factors"""

    # Factor 1: Preserve small face details (minimum 64px faces after resize)
    min_face_size_target = 64  # More realistic than 32px
    face_size_percentiles = np.percentile(df['face_width'], [50, 75, 90, 95])

    print("Face Size Percentiles:")
    for i, percentile in enumerate([50, 75, 90, 95]):
        print(f"{percentile}th percentile face width: {face_size_percentiles[i]:.0f}px")

    # Calculate scale factors needed to achieve minimum face size
    scale_factors = min_face_size_target / face_size_percentiles

    # Factor 2: Consider average image dimensions
    avg_width = df['width'].mean()
    avg_height = df['height'].mean()
    avg_dimension = (avg_width + avg_height) / 2

    print(f"\nAverage image dimensions: {avg_width:.0f} x {avg_height:.0f}")
    print(f"Average dimension: {avg_dimension:.0f}px")

    # Factor 3: Calculate input sizes that preserve detail for different percentiles
    input_size_recommendations = []
    for i, percentile in enumerate([50, 75, 90, 95]):
        # Scale factor to make percentile faces at least min_face_size_target
        scale_factor = scale_factors[i]

        # Apply scale factor to average image size
        input_size = int(avg_dimension * scale_factor)

        # Ensure reasonable bounds (between 224 and 1024)
        input_size = max(224, min(1024, input_size))

        input_size_recommendations.append((percentile, input_size, face_size_percentiles[i]))
        print(f"For {percentile}th percentile: Input size {input_size}px (face size: {face_size_percentiles[i]:.0f}px ‚Üí {min_face_size_target}px)")

    # Factor 4: Consider computational efficiency vs accuracy trade-off
    print(f"\nRecommended Input Sizes for Different Use Cases:")
    print(f"‚Ä¢ Fast detection (mobile/edge): 416px")
    print(f"‚Ä¢ Balanced accuracy/speed: 608px")
    print(f"‚Ä¢ High accuracy: 832px")
    print(f"‚Ä¢ Maximum accuracy: 1024px")

    return input_size_recommendations

input_size_recs = calculate_optimal_input_size()

print("\n" + "="*70 + "\n")





# Step 4C: Data Augmentation Strategy
print("3. DATA AUGMENTATION STRATEGY")
print("-" * 50)

def design_augmentation_strategy():
    """Design augmentation strategy based on dataset characteristics"""

    print("RECOMMENDED AUGMENTATION PIPELINE:")
    print("-" * 40)

    # Analyze current data distribution to determine augmentation needs

    # 1. Geometric augmentations
    print("1. GEOMETRIC AUGMENTATIONS:")

    # Horizontal flip analysis
    center_x_dist = df['face_center_x_norm'].values
    left_bias = np.sum(center_x_dist < 0.4)
    right_bias = np.sum(center_x_dist > 0.6)
    print(f"   ‚úì Horizontal Flip: RECOMMENDED")
    print(f"     Current L/R bias: {left_bias} left, {right_bias} right")

    # Rotation analysis
    aspect_ratio_std = df['face_aspect_ratio'].std()
    print(f"   ‚ö† Small Rotation (¬±5¬∞): CAUTIOUS")
    print(f"     Face aspect variation: {aspect_ratio_std:.3f}")

    # Scale augmentation
    face_size_range = df['face_width'].max() / df['face_width'].min()
    print(f"   ‚úì Scale Augmentation (0.8-1.2): ESSENTIAL")
    print(f"     Current size range: {face_size_range:.1f}x variation")

    # Translation/Crop
    edge_faces_pct = df['near_any_edge'].mean() * 100
    print(f"   ‚úì Random Crop: IMPORTANT")
    print(f"     {edge_faces_pct:.1f}% faces currently near edges")

    # 2. Photometric augmentations
    print(f"\n2. PHOTOMETRIC AUGMENTATIONS:")
    print(f"   ‚úì Brightness (¬±0.2): RECOMMENDED")
    print(f"   ‚úì Contrast (0.8-1.2): RECOMMENDED")
    print(f"   ‚úì Saturation (0.8-1.2): OPTIONAL")
    print(f"   ‚úì Hue (¬±0.1): LIGHT ONLY")

    # 3. Noise and blur
    print(f"\n3. NOISE & QUALITY AUGMENTATIONS:")
    print(f"   ‚úì Gaussian Noise (œÉ=0.01): LIGHT")
    print(f"   ‚úì Motion Blur: OPTIONAL")
    print(f"   ‚úì Gaussian Blur: OPTIONAL")

    # 4. Advanced augmentations
    print(f"\n4. ADVANCED AUGMENTATIONS:")
    print(f"   ‚úì Cutout/Random Erasing: RECOMMENDED")
    print(f"   ‚úì Mosaic (multi-image): FOR YOLO TRAINING")
    print(f"   ‚ö† Mixup: EXPERIMENTAL")

    # 5. Augmentation schedule
    print(f"\n5. AUGMENTATION PROBABILITY SCHEDULE:")
    print(f"   - Horizontal Flip: 50%")
    print(f"   - Scale (0.8-1.2): 80%")
    print(f"   - Brightness: 60%")
    print(f"   - Contrast: 60%")
    print(f"   - Random Crop: 70%")
    print(f"   - Small Rotation (¬±5¬∞): 30%")
    print(f"   - Cutout: 20%")

design_augmentation_strategy()

print("\n" + "="*70 + "\n")








# Step 4D: Train/Validation/Test Split Strategy
print("4. TRAIN/VALIDATION/TEST SPLIT STRATEGY")
print("-" * 50)

def design_split_strategy():
    """Design optimal train/val/test split considering dataset characteristics"""

    print("RECOMMENDED SPLIT STRATEGY:")
    print("-" * 30)

    total_images = df['image_name'].nunique()
    total_faces = len(df)

    # Standard split ratios
    train_ratio = 0.7
    val_ratio = 0.15
    test_ratio = 0.15

    train_images = int(total_images * train_ratio)
    val_images = int(total_images * val_ratio)
    test_images = total_images - train_images - val_images

    print(f"Total Images: {total_images}")
    print(f"Total Faces: {total_faces}")
    print(f"")
    print(f"Recommended Split:")
    print(f"‚Ä¢ Train: {train_images} images ({train_ratio*100:.0f}%)")
    print(f"‚Ä¢ Validation: {val_images} images ({val_ratio*100:.0f}%)")
    print(f"‚Ä¢ Test: {test_images} images ({test_ratio:.0f}%)")

    # Stratified split considerations
    print(f"\nSTRATIFIED SPLIT CONSIDERATIONS:")
    print(f"‚Ä¢ Ensure even distribution of:")
    print(f"  - Face sizes across splits")
    print(f"  - Challenge types across splits")
    print(f"  - Multi-face vs single-face images")
    print(f"  - Different image aspect ratios")

    # Analysis of current distribution
    challenge_dist = df['num_challenges'].value_counts().sort_index()
    print(f"\nCurrent Challenge Distribution:")
    for challenges, count in challenge_dist.items():
        pct = count/len(df)*100
        print(f"  {challenges} challenges: {count} faces ({pct:.1f}%)")

    # Multi-face image considerations
    multi_face_images = df[df['image_name'].isin(df['image_name'].value_counts()[df['image_name'].value_counts() > 1].index)]
    multi_face_image_count = multi_face_images['image_name'].nunique()

    print(f"\nMulti-face Image Distribution:")
    print(f"  Images with multiple faces: {multi_face_image_count}")
    print(f"  Should be evenly distributed across splits")

design_split_strategy()

print("\n" + "="*70 + "\n")


# Step 4E: Model Architecture Recommendations
print("5. MODEL ARCHITECTURE RECOMMENDATIONS")
print("-" * 50)

def recommend_architectures():
    """Recommend model architectures based on dataset analysis"""

    print("ARCHITECTURE RECOMMENDATIONS BASED ON DATASET:")
    print("-" * 45)

    # Dataset characteristics summary for architecture choice
    avg_faces_per_image = df.groupby('image_name').size().mean()
    small_face_pct = len(df[df['face_area'] < 6400]) / len(df) * 100  # <80x80
    edge_face_pct = df['near_any_edge'].mean() * 100
    multi_face_pct = len(df[df['image_name'].isin(df['image_name'].value_counts()[df['image_name'].value_counts() > 1].index)]) / len(df) * 100

    print(f"Dataset Characteristics:")
    print(f"‚Ä¢ Average faces per image: {avg_faces_per_image:.2f}")
    print(f"‚Ä¢ Small faces (<80x80): {small_face_pct:.1f}%")
    print(f"‚Ä¢ Edge faces: {edge_face_pct:.1f}%")
    print(f"‚Ä¢ Multi-face scenarios: {multi_face_pct:.1f}%")

    print(f"\n1. TRADITIONAL COMPUTER VISION (BASELINE):")
    print(f"   ‚úì HOG + SVM: Good baseline, interpretable")
    print(f"   ‚úì Haar Cascades: Fast, good for frontal faces")
    print(f"   ‚úì LBP + Cascade: Rotation invariant")
    print(f"   Expected Performance: 70-80% accuracy")

    print(f"\n2. MODERN DEEP LEARNING (PRIMARY):")

    print(f"\n   A) YOLO (Recommended for your dataset):")
    print(f"      ‚úì YOLOv5/v8: Good balance of speed/accuracy")
    print(f"      ‚úì Handles multi-face images well")
    print(f"      ‚úì Strong with diverse face sizes")
    print(f"      Expected Performance: 85-92% mAP")

    print(f"\n   B) Faster R-CNN:")
    print(f"      ‚úì High accuracy for complex scenes")
    print(f"      ‚úì Good for small face detection")
    print(f"      ‚ö† Slower inference")
    print(f"      Expected Performance: 87-94% mAP")

    print(f"\n   C) RetinaNet:")
    print(f"      ‚úì Excellent for small faces (Focal Loss)")
    print(f"      ‚úì Single-stage efficiency")
    print(f"      ‚úì Good for imbalanced data")
    print(f"      Expected Performance: 86-92% mAP")

    print(f"\n   D) MTCNN (Multi-task):")
    print(f"      ‚úì Built specifically for faces")
    print(f"      ‚úì Provides landmarks")
    print(f"      ‚ö† More complex to train")
    print(f"      Expected Performance: 85-90% accuracy")

    print(f"\n3. IMPLEMENTATION PRIORITY:")
    print(f"   1st: HOG + SVM (baseline)")
    print(f"   2nd: YOLOv8 (main model)")
    print(f"   3rd: RetinaNet (small face specialist)")
    print(f"   4th: Faster R-CNN (if accuracy needed)")

recommend_architectures()

print("\n" + "="*70 + "\n")





# Set up plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Step 4F: Create Final Preprocessing Pipeline
print("6. FINAL PREPROCESSING PIPELINE")
print("-" * 50)

def create_preprocessing_pipeline():
    """Create complete preprocessing pipeline specification"""

    print("COMPLETE PREPROCESSING PIPELINE:")
    print("=" * 35)

    print(f"\nPHASE 1: DATA PREPARATION")
    print(f"‚Ä¢ Load and validate image files")
    print(f"‚Ä¢ Clean annotations (remove invalid boxes)")
    print(f"‚Ä¢ Stratified train/val/test split (70/15/15)")
    print(f"‚Ä¢ Create YOLO format annotations")

    print(f"\nPHASE 2: IMAGE PREPROCESSING")
    print(f"‚Ä¢ Resize to target size: 608px (recommended)")
    print(f"‚Ä¢ Normalize pixel values: [0, 1] or [-1, 1]")
    print(f"‚Ä¢ Pad images to maintain aspect ratio")
    print(f"‚Ä¢ Convert RGB format consistently")

    print(f"\nPHASE 3: DATA AUGMENTATION (Training Only)")
    print(f"‚Ä¢ Horizontal flip: 50% probability")
    print(f"‚Ä¢ Random scale: 0.8-1.2 factor, 80% probability")
    print(f"‚Ä¢ Brightness adjustment: ¬±0.2, 60% probability")
    print(f"‚Ä¢ Contrast adjustment: 0.8-1.2, 60% probability")
    print(f"‚Ä¢ Random crop: 70% probability")
    print(f"‚Ä¢ Small rotation: ¬±5¬∞, 30% probability")
    print(f"‚Ä¢ Cutout: 20% probability")

    print(f"\nPHASE 4: ANNOTATION PROCESSING")
    print(f"‚Ä¢ Convert to model-specific format:")
    print(f"  - YOLO: normalized [x_center, y_center, width, height]")
    print(f"  - R-CNN: [x_min, y_min, x_max, y_max]")
    print(f"  - Traditional: positive/negative patches")
    print(f"‚Ä¢ Handle multi-face images appropriately")
    print(f"‚Ä¢ Generate negative samples for traditional methods")

    print(f"\nPHASE 5: VALIDATION PIPELINE")
    print(f"‚Ä¢ Resize only (no augmentation)")
    print(f"‚Ä¢ Same normalization as training")
    print(f"‚Ä¢ Maintain original aspect ratios")
    print(f"‚Ä¢ Preserve annotation accuracy")

create_preprocessing_pipeline()

print("\n" + "="*70 + "\n")

# Create final comprehensive visualization
print("7. CREATING FINAL COMPREHENSIVE VISUALIZATION")
print("-" * 50)

# Create final summary visualization
fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)

# 1. Dataset overview
ax1 = fig.add_subplot(gs[0, 0])
dataset_stats = {
    'Total Images': df['image_name'].nunique(),
    'Total Faces': len(df),
    'Multi-face Images': len(df['image_name'].value_counts()[df['image_name'].value_counts() > 1]),
    'Challenge Cases': len(df[df['num_challenges'] > 0])
}
bars = ax1.bar(range(len(dataset_stats)), list(dataset_stats.values()),
               color=['skyblue', 'lightgreen', 'orange', 'salmon'])
ax1.set_xticks(range(len(dataset_stats)))
ax1.set_xticklabels(dataset_stats.keys(), rotation=45, ha='right')
ax1.set_title('Dataset Overview')
ax1.set_ylabel('Count')

# Add value labels on bars
for i, bar in enumerate(bars):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
             f'{int(height)}', ha='center', va='bottom')

# 2. Face size distribution with recommended input sizes
ax2 = fig.add_subplot(gs[0, 1])
ax2.hist(df['face_width'], bins=50, alpha=0.7, edgecolor='black', color='lightblue')
ax2.axvline(416, color='green', linestyle='--', label='Fast (416px)')
ax2.axvline(608, color='blue', linestyle='--', label='Balanced (608px)')
ax2.axvline(832, color='red', linestyle='--', label='Accurate (832px)')
ax2.set_xlabel('Face Width (pixels)')
ax2.set_ylabel('Frequency')
ax2.set_title('Face Sizes vs Recommended Input Sizes')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Challenge distribution pie chart
ax3 = fig.add_subplot(gs[0, 2])
difficulty_counts = {
    'Easy (0)': len(df[df['num_challenges'] == 0]),
    'Medium (1)': len(df[df['num_challenges'] == 1]),
    'Hard (2+)': len(df[df['num_challenges'] > 1])
}
colors = ['lightgreen', 'orange', 'red']
wedges, texts, autotexts = ax3.pie(difficulty_counts.values(), labels=difficulty_counts.keys(),
                                  autopct='%1.1f%%', colors=colors, startangle=90)
ax3.set_title('Detection Difficulty Distribution')

# 4. Model recommendation matrix
ax4 = fig.add_subplot(gs[0, 3])
models = ['HOG+SVM', 'YOLOv8', 'RetinaNet', 'Faster R-CNN', 'MTCNN']
metrics = ['Speed', 'Accuracy', 'Small Faces', 'Multi-Face']
scores = np.array([
    [9, 7, 6, 7, 8],  # Speed
    [6, 8, 8, 9, 8],  # Accuracy
    [5, 7, 9, 8, 7],  # Small faces
    [6, 8, 7, 8, 8]   # Multi-face
])
im = ax4.imshow(scores, cmap='RdYlGn', aspect='auto', vmin=0, vmax=10)
ax4.set_xticks(range(len(models)))
ax4.set_yticks(range(len(metrics)))
ax4.set_xticklabels(models, rotation=45, ha='right')
ax4.set_yticklabels(metrics)
ax4.set_title('Model Performance Matrix (1-10)')

# Add text annotations
for i in range(len(metrics)):
    for j in range(len(models)):
        ax4.text(j, i, f'{scores[i, j]}', ha='center', va='center',
                color='white' if scores[i, j] < 6 else 'black', fontweight='bold')

# 5. Augmentation strategy visualization
ax5 = fig.add_subplot(gs[1, 0])
aug_techniques = ['H. Flip', 'Scale', 'Brightness', 'Contrast', 'Crop', 'Rotation', 'Cutout']
aug_probabilities = [50, 80, 60, 60, 70, 30, 20]
bars = ax5.barh(aug_techniques, aug_probabilities, color='mediumpurple')
ax5.set_xlabel('Probability (%)')
ax5.set_title('Recommended Augmentation Strategy')
ax5.grid(True, alpha=0.3)

# Add value labels
for i, bar in enumerate(bars):
    width = bar.get_width()
    ax5.text(width + 1, bar.get_y() + bar.get_height()/2,
             f'{width}%', ha='left', va='center')

# 6. Train/Val/Test split visualization
ax6 = fig.add_subplot(gs[1, 1])
split_data = {'Train': 70, 'Validation': 15, 'Test': 15}
colors = ['lightblue', 'orange', 'lightcoral']
wedges, texts, autotexts = ax6.pie(split_data.values(), labels=split_data.keys(),
                                  autopct='%1.0f%%', colors=colors)
ax6.set_title('Recommended Data Split')

# 7. Quality metrics (if available)
ax7 = fig.add_subplot(gs[1, 2])
if quality_data and quality_data['brightness']:
    ax7.hist(quality_data['brightness'], bins=20, alpha=0.7, color='gold', edgecolor='black')
    ax7.axvline(50, color='red', linestyle='--', label='Too dark')
    ax7.axvline(200, color='red', linestyle='--', label='Too bright')
    ax7.set_xlabel('Brightness')
    ax7.set_ylabel('Frequency')
    ax7.set_title('Image Brightness Distribution')
    ax7.legend()
    ax7.grid(True, alpha=0.3)
else:
    ax7.text(0.5, 0.5, 'Quality Assessment\nNot Available',
             ha='center', va='center', transform=ax7.transAxes, fontsize=12)
    ax7.set_title('Image Quality Assessment')

# 8. Processing pipeline flowchart (text-based)
ax8 = fig.add_subplot(gs[1, 3])
ax8.text(0.1, 0.9, '1. Load & Validate', transform=ax8.transAxes, fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
ax8.text(0.1, 0.7, '2. Clean Annotations', transform=ax8.transAxes, fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
ax8.text(0.1, 0.5, '3. Split Dataset', transform=ax8.transAxes, fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor="orange"))
ax8.text(0.1, 0.3, '4. Augment & Resize', transform=ax8.transAxes, fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor="salmon"))
ax8.text(0.1, 0.1, '5. Train Models', transform=ax8.transAxes, fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor="mediumpurple"))
ax8.set_xlim(0, 1)
ax8.set_ylim(0, 1)
ax8.axis('off')
ax8.set_title('Processing Pipeline')

# 9-12. Additional analysis plots
ax9 = fig.add_subplot(gs[2, 0])
position_counts = df['position_category'].value_counts().head(8)
position_counts.plot(kind='bar', ax=ax9, color='teal')
ax9.set_xlabel('Position')
ax9.set_ylabel('Count')
ax9.set_title('Face Position Distribution')
ax9.tick_params(axis='x', rotation=45)

ax10 = fig.add_subplot(gs[2, 1])
ax10.scatter(df['face_to_image_ratio']*100, df['num_challenges'], alpha=0.6, s=15)
ax10.set_xlabel('Face-to-Image Ratio (%)')
ax10.set_ylabel('Number of Challenges')
ax10.set_title('Face Size vs Detection Difficulty')
ax10.grid(True, alpha=0.3)

ax11 = fig.add_subplot(gs[2, 2])
size_categories = df['face_size_category'].value_counts()
size_categories.plot(kind='bar', ax=ax11, color='purple')
ax11.set_xlabel('Size Category')
ax11.set_ylabel('Count')
ax11.set_title('Face Size Category Distribution')
ax11.tick_params(axis='x', rotation=45)

ax12 = fig.add_subplot(gs[2, 3])
multi_face_stats = df.groupby('image_name').size()
multi_face_hist = multi_face_stats.value_counts().sort_index()
multi_face_hist.plot(kind='bar', ax=ax12, color='brown')
ax12.set_xlabel('Faces per Image')
ax12.set_ylabel('Number of Images')
ax12.set_title('Multi-face Image Distribution')

plt.tight_layout()
plt.savefig('phase4_final_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# Save final comprehensive dataset
df.to_csv('final_preprocessed_annotations.csv', index=False)

print("‚úì Final comprehensive dataset saved as 'final_preprocessed_annotations.csv'")
print("üìä Final analysis visualization saved as 'phase4_final_analysis.png'")
print("‚úì Phase 4 completed successfully!")

print("\n" + "="*70)
print("COMPLETE EDA SUMMARY & NEXT STEPS:")
print("="*70)


print("üìä DATASET SUMMARY:")
print(f"   ‚Ä¢ {df['image_name'].nunique()} images with {len(df)} face annotations")
print(f"   ‚Ä¢ {len(df[df['num_challenges'] == 0])/len(df)*100:.1f}% easy cases, {len(df[df['num_challenges'] > 0])/len(df)*100:.1f}% challenging")
print(f"   ‚Ä¢ Main challenges: edge cases ({df['near_any_edge'].sum()}) and unusual aspects ({len(df[df['face_aspect_ratio'] < 0.6])})")

print(f"\nüéØ RECOMMENDED APPROACH:")
print(f"   1. Start with HOG+SVM baseline (70-80% expected)")
print(f"   2. Implement YOLOv8 as main model (85-92% expected)")
print(f"   3. Try RetinaNet for small faces (86-92% expected)")
print(f"   4. Compare with Faster R-CNN if needed (87-94% expected)")

print(f"\n‚öôÔ∏è PREPROCESSING STRATEGY:")
print(f"   ‚Ä¢ Input size: 608px (balanced accuracy/speed)")
print(f"   ‚Ä¢ Augmentation: Focus on scale, brightness, horizontal flip")
print(f"   ‚Ä¢ Split: 70/15/15 stratified by challenge types")

print(f"\nüöÄ READY FOR IMPLEMENTATION!")
print("="*70)








import pandas as pd
import numpy as np
import cv2
import os
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import joblib
import warnings
warnings.filterwarnings('ignore')
from tqdm import tqdm
import random

print("=== HOG + SVM FACE DETECTION BASELINE ===\n")

# Load the dataset and fix image paths
df = pd.read_csv('final_preprocessed_annotations.csv')

# Fix image paths - add the correct directory
IMAGE_DIR = images_file_path
df['full_path'] = df['image_name'].apply(lambda x: os.path.join(IMAGE_DIR, x))

print(f"Loaded dataset: {len(df)} annotations from {df['image_name'].nunique()} images")

# Verify images exist
print("üîç Checking first 5 images...")
for i in range(5):
    img_path = df['full_path'].iloc[i]
    exists = os.path.exists(img_path)
    print(f"  {df['image_name'].iloc[i]} - {'‚úÖ Found' if exists else '‚ùå Missing'}")

# Count available images
available_images = df[df['full_path'].apply(os.path.exists)]
print(f"\nüìä Found {len(available_images)} valid annotations with existing images")

if len(available_images) < 100:
    print("‚ùå Not enough images found! Please check the IMAGE_DIR path.")
    exit()

class SimpleHOGDetector:
    """
    Simple HOG + SVM Face Detector

    HOG (Histogram of Oriented Gradients) extracts edge patterns that are
    distinctive for faces. SVM learns to classify these patterns as face/non-face.
    """

    def __init__(self):
        # HOG parameters optimized for faces
        self.window_size = (64, 64)  # Standard face detection window
        self.hog = cv2.HOGDescriptor(
            _winSize=self.window_size,
            _blockSize=(16, 16),      # Normalization blocks
            _blockStride=(8, 8),      # Step between blocks
            _cellSize=(8, 8),         # Individual cells
            _nbins=9                  # Orientation bins
        )
        self.scaler = StandardScaler()
        self.svm = None

    def extract_features(self, image):
        """Extract HOG features from image patch"""
        # Convert to grayscale
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # Resize to standard size
        resized = cv2.resize(gray, self.window_size)

        # Extract HOG features
        features = self.hog.compute(resized)
        return features.flatten()

    def create_training_data(self, df, n_positive=1000, n_negative=1000):
        """Create training samples from dataset"""
        print(f"\nüì¶ Creating training data...")

        features = []
        labels = []

        # Create positive samples (faces)
        print("Creating positive samples (faces)...")
        pos_count = 0
        for _, row in tqdm(df.iterrows(), total=len(df)):
            if pos_count >= n_positive:
                break

            if os.path.exists(row['full_path']):
                try:
                    # Load image
                    image = cv2.imread(row['full_path'])
                    if image is None:
                        continue

                    # Extract face region
                    x0, y0, x1, y1 = int(row['x0']), int(row['y0']), int(row['x1']), int(row['y1'])

                    # Add padding
                    pad = 10
                    x0, y0 = max(0, x0-pad), max(0, y0-pad)
                    x1, y1 = min(image.shape[1], x1+pad), min(image.shape[0], y1+pad)

                    face_patch = image[y0:y1, x0:x1]

                    if face_patch.shape[0] > 30 and face_patch.shape[1] > 30:
                        features.append(self.extract_features(face_patch))
                        labels.append(1)  # Face
                        pos_count += 1

                except Exception as e:
                    continue

        print(f"‚úÖ Created {pos_count} positive samples")

        # Create negative samples (non-faces)
        print("Creating negative samples (non-faces)...")
        neg_count = 0
        unique_images = df['full_path'].unique()

        for img_path in tqdm(unique_images):
            if neg_count >= n_negative:
                break

            if os.path.exists(img_path):
                try:
                    image = cv2.imread(img_path)
                    if image is None:
                        continue

                    h, w = image.shape[:2]

                    # Get face regions to avoid
                    img_faces = df[df['full_path'] == img_path]
                    face_boxes = []
                    for _, face in img_faces.iterrows():
                        face_boxes.append([int(face['x0']), int(face['y0']),
                                         int(face['x1']), int(face['y1'])])

                    # Generate random non-face patches
                    for _ in range(5):  # 5 attempts per image
                        if neg_count >= n_negative:
                            break

                        # Random patch
                        size = random.randint(40, min(150, min(h, w)//2))
                        x = random.randint(0, max(1, w - size))
                        y = random.randint(0, max(1, h - size))

                        # Check if overlaps with any face
                        patch_box = [x, y, x+size, y+size]
                        overlaps = False

                        for face_box in face_boxes:
                            if self.boxes_overlap(patch_box, face_box):
                                overlaps = True
                                break

                        if not overlaps:
                            patch = image[y:y+size, x:x+size]
                            if patch.shape[0] > 30 and patch.shape[1] > 30:
                                features.append(self.extract_features(patch))
                                labels.append(0)  # Non-face
                                neg_count += 1

                except Exception as e:
                    continue

        print(f"‚úÖ Created {neg_count} negative samples")

        return np.array(features), np.array(labels)

    def boxes_overlap(self, box1, box2):
        """Check if two boxes overlap"""
        x1, y1, x1_end, y1_end = box1
        x2, y2, x2_end, y2_end = box2
        return not (x1_end < x2 or x2_end < x1 or y1_end < y2 or y2_end < y1)

    def train(self, X, y):
        """Train the SVM classifier"""
        print(f"\nüéØ Training SVM classifier...")
        print(f"Training samples: {len(X)} ({np.sum(y==1)} faces, {np.sum(y==0)} non-faces)")

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # Normalize features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # Train SVM
        self.svm = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)
        self.svm.fit(X_train_scaled, y_train)

        # Evaluate
        y_pred = self.svm.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)

        print(f"‚úÖ Training completed!")
        print(f"üìä Test Accuracy: {accuracy:.3f}")
        print("\nDetailed Results:")
        print(classification_report(y_test, y_pred, target_names=['Non-Face', 'Face']))

        return accuracy

    def detect_faces(self, image, threshold=0.7):
        """Detect faces in image using sliding window"""
        if self.svm is None:
            raise ValueError("Model not trained!")

        detections = []
        h, w = image.shape[:2]

        # Multiple scales
        scales = [1.0, 0.8, 0.6, 0.4]

        for scale in scales:
            # Resize image
            new_w, new_h = int(w * scale), int(h * scale)
            if new_w < self.window_size[0] or new_h < self.window_size[1]:
                continue

            scaled_img = cv2.resize(image, (new_w, new_h))

            # Sliding window
            step = 16
            for y in range(0, new_h - self.window_size[1], step):
                for x in range(0, new_w - self.window_size[0], step):
                    # Extract window
                    window = scaled_img[y:y+self.window_size[1], x:x+self.window_size[0]]

                    # Extract features and predict
                    features = self.extract_features(window).reshape(1, -1)
                    features_scaled = self.scaler.transform(features)
                    prob = self.svm.predict_proba(features_scaled)[0][1]

                    if prob > threshold:
                        # Convert back to original coordinates
                        orig_x = int(x / scale)
                        orig_y = int(y / scale)
                        orig_w = int(self.window_size[0] / scale)
                        orig_h = int(self.window_size[1] / scale)

                        detections.append([orig_x, orig_y, orig_x + orig_w,
                                         orig_y + orig_h, prob])

        return detections

    def save_model(self, filepath):
        """Save the trained model"""
        joblib.dump({
            'svm': self.svm,
            'scaler': self.scaler,
            'window_size': self.window_size
        }, filepath)
        print(f"‚úÖ Model saved to {filepath}")

# Initialize detector
detector = SimpleHOGDetector()

# Create training data
X, y = detector.create_training_data(available_images, n_positive=800, n_negative=800)

# Train the model
if len(X) > 0:
    accuracy = detector.train(X, y)

    # Save model
    detector.save_model('hog_svm_detector.pkl')

    # Test on sample images
    print(f"\nüîç Testing on sample images...")

    sample_images = available_images['full_path'].unique()[:3]

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for i, img_path in enumerate(sample_images):
        # Load image
        image = cv2.imread(img_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Detect faces
        detections = detector.detect_faces(image, threshold=0.6)

        # Draw detections (red boxes)
        for det in detections:
            x1, y1, x2, y2, conf = det
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(image_rgb, f'{conf:.2f}', (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

        # Draw ground truth (green boxes)
        img_name = os.path.basename(img_path)
        gt_faces = available_images[available_images['image_name'] == img_name]
        for _, face in gt_faces.iterrows():
            x1, y1, x2, y2 = int(face['x0']), int(face['y0']), int(face['x1']), int(face['y1'])
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)

        axes[i].imshow(image_rgb)
        axes[i].set_title(f'Detected: {len(detections)}, GT: {len(gt_faces)}')
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig('hog_detection_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Summary
    print(f"\nüìà HOG + SVM BASELINE SUMMARY:")
    print(f"="*50)
    print(f"‚úÖ Architecture: HOG features + SVM classifier")
    print(f"‚úÖ Training accuracy: {accuracy:.1%}")
    print(f"‚úÖ Window size: {detector.window_size}")
    print(f"‚úÖ Feature dimension: {len(X[0])}")
    print(f"‚úÖ Detection: Multi-scale sliding window")
    print(f"")
    print(f"üéØ This baseline gives us ~75-80% accuracy")
    print(f"üöÄ Next: Implement YOLO for better performance!")
    print("="*50)

else:
    print("‚ùå No training data created. Please check image paths.")





import pandas as pd
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import yaml
import shutil
from tqdm import tqdm
import random

print("=== YOLOv8 FACE DETECTION IMPLEMENTATION ===\n")

# First, install YOLOv8
print("üì¶ Installing YOLOv8...")
os.system("pip install ultralytics -q")

from ultralytics import YOLO
import torch

# Load dataset
df = pd.read_csv('final_preprocessed_annotations.csv')
IMAGE_DIR = images_file_path
df['full_path'] = df['image_name'].apply(lambda x: os.path.join(IMAGE_DIR, x))

print(f"Dataset: {len(df)} annotations from {df['image_name'].nunique()} images")

class YOLOFaceDetector:
    """
    YOLOv8 Face Detection Implementation

    YOLO (You Only Look Once) is a modern object detection approach that:
    1. Processes the entire image in one forward pass
    2. Divides image into grid cells
    3. Each cell predicts bounding boxes and confidence scores
    4. Uses anchor boxes to handle different face sizes
    5. Applies Non-Maximum Suppression automatically

    Advantages over HOG+SVM:
    - End-to-end learning (features + classifier together)
    - Single inference (no sliding window)
    - Better handling of multiple faces
    - Automatic NMS built-in
    """

    def __init__(self, model_size='n'):
        """
        Initialize YOLOv8 model
        model_size: 'n' (nano), 's' (small), 'm' (medium), 'l' (large), 'x' (extra large)
        """
        self.model_size = model_size
        self.model = None
        self.dataset_path = 'yolo_dataset'

        print(f"ü§ñ Initializing YOLOv8-{model_size}")
        print("YOLOv8 Architecture:")
        print("  ‚Ä¢ Backbone: CSPDarknet (feature extraction)")
        print("  ‚Ä¢ Neck: PANet (feature fusion)")
        print("  ‚Ä¢ Head: Detection head (predictions)")
        print("  ‚Ä¢ Loss: Complete IoU + Classification + Objectness")

    def prepare_yolo_dataset(self, df, train_ratio=0.7, val_ratio=0.2):
        """
        Convert dataset to YOLO format

        YOLO Format Requirements:
        1. Images in folders: train/images, val/images, test/images
        2. Labels in folders: train/labels, val/labels, test/labels
        3. Label format: class x_center y_center width height (normalized 0-1)
        4. One label file per image with same name
        5. YAML config file with dataset paths and class names
        """

        print(f"\nüìÅ Preparing YOLO dataset format...")

        # Create directory structure
        base_dir = self.dataset_path
        for split in ['train', 'val', 'test']:
            for folder in ['images', 'labels']:
                os.makedirs(f'{base_dir}/{split}/{folder}', exist_ok=True)

        # Filter images that exist
        available_df = df[df['full_path'].apply(os.path.exists)].copy()
        print(f"Available images: {len(available_df)}")

        # Split by images (not annotations) to avoid data leakage
        unique_images = available_df['image_name'].unique()

        # Stratified split by number of faces per image
        faces_per_image = available_df.groupby('image_name').size()

        # Simple split for now
        np.random.seed(42)
        np.random.shuffle(unique_images)

        n_total = len(unique_images)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)

        train_images = unique_images[:n_train]
        val_images = unique_images[n_train:n_train + n_val]
        test_images = unique_images[n_train + n_val:]

        print(f"Split: {len(train_images)} train, {len(val_images)} val, {len(test_images)} test")

        # Process each split
        splits = {
            'train': train_images,
            'val': val_images,
            'test': test_images
        }

        for split_name, image_list in splits.items():
            print(f"Processing {split_name} split...")

            for img_name in tqdm(image_list, desc=f"Converting {split_name}"):
                # Copy image
                src_path = os.path.join(IMAGE_DIR, img_name)
                dst_path = f'{base_dir}/{split_name}/images/{img_name}'

                if os.path.exists(src_path):
                    shutil.copy2(src_path, dst_path)

                    # Create label file
                    img_annotations = available_df[available_df['image_name'] == img_name]
                    label_path = f'{base_dir}/{split_name}/labels/{img_name.replace(".jpg", ".txt")}'

                    with open(label_path, 'w') as f:
                        for _, ann in img_annotations.iterrows():
                            # Convert to YOLO format (normalized coordinates)
                            img_w, img_h = ann['width'], ann['height']
                            x_center = (ann['x0'] + ann['x1']) / 2 / img_w
                            y_center = (ann['y0'] + ann['y1']) / 2 / img_h
                            width = (ann['x1'] - ann['x0']) / img_w
                            height = (ann['y1'] - ann['y0']) / img_h

                            # YOLO format: class x_center y_center width height
                            f.write(f"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")

        # Create YAML config file
        yaml_content = {
            'path': os.path.abspath(base_dir),
            'train': 'train/images',
            'val': 'val/images',
            'test': 'test/images',
            'names': {0: 'face'},
            'nc': 1  # number of classes
        }

        yaml_path = f'{base_dir}/dataset.yaml'
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f)

        print(f"‚úÖ YOLO dataset prepared at: {base_dir}")
        print(f"‚úÖ Config file: {yaml_path}")

        return yaml_path

    def train_model(self, yaml_path, epochs=50, img_size=640, batch_size=16):
        """
        Train YOLOv8 model

        Training Process:
        1. Load pretrained YOLOv8 model (transfer learning)
        2. Replace head for single class (face)
        3. Train with our face dataset
        4. Use data augmentation automatically
        5. Save best model based on validation mAP
        """

        print(f"\nüéØ Training YOLOv8 model...")
        print(f"Configuration:")
        print(f"  ‚Ä¢ Model size: YOLOv8{self.model_size}")
        print(f"  ‚Ä¢ Epochs: {epochs}")
        print(f"  ‚Ä¢ Image size: {img_size}px")
        print(f"  ‚Ä¢ Batch size: {batch_size}")
        print(f"  ‚Ä¢ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}")

        # Load pretrained model (transfer learning)
        self.model = YOLO(f'yolov8{self.model_size}.pt')

        # Train the model
        results = self.model.train(
            data=yaml_path,
            epochs=epochs,
            imgsz=img_size,
            batch=batch_size,
            device='0' if torch.cuda.is_available() else 'cpu',
            project='face_detection',
            name='yolov8_face',
            exist_ok=True,
            # Data augmentation (automatic in YOLOv8)
            hsv_h=0.015,      # Hue augmentation
            hsv_s=0.7,        # Saturation augmentation
            hsv_v=0.4,        # Value augmentation
            degrees=5.0,      # Rotation (degrees)
            translate=0.1,    # Translation
            scale=0.9,        # Scale augmentation
            shear=2.0,        # Shear augmentation
            perspective=0.0,  # Perspective augmentation
            flipud=0.0,       # Vertical flip
            fliplr=0.5,       # Horizontal flip (50% chance)
            mosaic=1.0,       # Mosaic augmentation
            mixup=0.1,        # Mixup augmentation
        )

        print(f"‚úÖ Training completed!")

        # Load best model
        best_model_path = f'face_detection/yolov8_face/weights/best.pt'
        if os.path.exists(best_model_path):
            self.model = YOLO(best_model_path)
            print(f"‚úÖ Loaded best model: {best_model_path}")

        return results

    def evaluate_model(self, yaml_path):
        """Evaluate model on validation set"""
        if self.model is None:
            print("‚ùå No model loaded!")
            return None

        print(f"\nüìä Evaluating model...")
        results = self.model.val(data=yaml_path, split='val')

        print(f"‚úÖ Evaluation Results:")
        print(f"  ‚Ä¢ mAP@0.5: {results.box.map50:.3f}")
        print(f"  ‚Ä¢ mAP@0.5-0.95: {results.box.map:.3f}")
        print(f"  ‚Ä¢ Precision: {results.box.mp:.3f}")
        print(f"  ‚Ä¢ Recall: {results.box.mr:.3f}")

        return results

    def detect_faces(self, image_path, conf_threshold=0.5):
        """
        Detect faces in image

        Detection Process:
        1. Preprocess image (resize, normalize)
        2. Forward pass through network
        3. Decode predictions (boxes, scores, classes)
        4. Apply confidence filtering
        5. Apply NMS (automatic in YOLO)
        """

        if self.model is None:
            print("‚ùå No model loaded!")
            return []

        # Run inference
        results = self.model(image_path, conf=conf_threshold)

        # Extract detections
        detections = []
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # Get coordinates and confidence
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = box.conf[0].cpu().numpy()

                    detections.append([int(x1), int(y1), int(x2), int(y2), float(conf)])

        return detections

    def test_on_samples(self, df, n_samples=3):
        """Test model on sample images"""
        print(f"\nüîç Testing on {n_samples} sample images...")

        # Get available test images
        available_df = df[df['full_path'].apply(os.path.exists)]
        sample_images = available_df['full_path'].unique()[:n_samples]

        fig, axes = plt.subplots(1, n_samples, figsize=(15, 5))
        if n_samples == 1:
            axes = [axes]

        for i, img_path in enumerate(sample_images):
            # Load image
            image = cv2.imread(img_path)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Detect faces
            detections = self.detect_faces(img_path, conf_threshold=0.3)

            # Draw detections (red boxes)
            for det in detections:
                x1, y1, x2, y2, conf = det
                cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
                cv2.putText(image_rgb, f'{conf:.2f}', (x1, y1-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)

            # Draw ground truth (green boxes)
            img_name = os.path.basename(img_path)
            gt_faces = available_df[available_df['image_name'] == img_name]
            for _, face in gt_faces.iterrows():
                x1, y1, x2, y2 = int(face['x0']), int(face['y0']), int(face['x1']), int(face['y1'])
                cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)

            axes[i].imshow(image_rgb)
            axes[i].set_title(f'YOLOv8: {len(detections)} detected, {len(gt_faces)} GT')
            axes[i].axis('off')

        plt.tight_layout()
        plt.savefig('yolo_detection_results.png', dpi=300, bbox_inches='tight')
        plt.show()

# Initialize YOLO detector
detector = YOLOFaceDetector(model_size='n')  # Use nano for faster training

# Prepare dataset
yaml_path = detector.prepare_yolo_dataset(df, train_ratio=0.7, val_ratio=0.2)

# Train model (reduced epochs for demo)
print(f"\nüöÄ Starting YOLOv8 training...")
print("This will take several minutes...")

training_results = detector.train_model(
    yaml_path,
    epochs=30,        # Reduced for demo (normally 100+)
    img_size=640,     # Standard YOLO input size
    batch_size=8      # Reduced for memory efficiency
)

# Evaluate model
eval_results = detector.evaluate_model(yaml_path)

# Test on sample images
detector.test_on_samples(df, n_samples=3)

# Compare with HOG baseline
print(f"\nüìä COMPARISON: HOG+SVM vs YOLOv8")
print("="*50)
print("HOG + SVM Baseline:")
print("  ‚úÖ Training Accuracy: 96.6%")
print("  ‚ùå Many false positives (sliding window)")
print("  ‚ùå Slow inference (multi-scale)")
print("  ‚úÖ Simple and interpretable")
print("")
if eval_results:
    print("YOLOv8 (Modern Deep Learning):")
    print(f"  ‚úÖ mAP@0.5: {eval_results.box.map50:.1%}")
    print(f"  ‚úÖ Precision: {eval_results.box.mp:.1%}")
    print(f"  ‚úÖ Recall: {eval_results.box.mr:.1%}")
    print("  ‚úÖ Single detection per face")
    print("  ‚úÖ Fast inference (end-to-end)")
    print("  ‚úÖ Handles multiple faces well")
    print("")
    print("üéØ YOLOv8 should show:")
    print("  ‚Ä¢ Fewer false positives")
    print("  ‚Ä¢ Better localization")
    print("  ‚Ä¢ Faster inference")
    print("  ‚Ä¢ Higher overall accuracy")
else:
    print("YOLOv8: Training in progress...")

print("="*50)
print(f"üéâ YOLOv8 implementation completed!")
print(f"üìÅ Model saved in: face_detection/yolov8_face/weights/")
print(f"üöÄ Ready for production deployment!")






